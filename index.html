<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="noindex" />
  <title>PRISM: Performer RS-IMLE for Multisensory Imitation Learning</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 2rem;
      line-height: 1.6;
      background: #f9f9f9;
      color: #333;
    }
    h1, h2 {
      color: #111;
    }
    .container {
      max-width: 960px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0,0,0,0.05);
    }
    .video-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
      gap: 1rem;
    }
    video, iframe {
      width: 100%;
      height: auto;
      border-radius: 6px;
    }
    img {
      max-width: 100%;
      border-radius: 6px;
    }
    pre {
      background: #eee;
      padding: 1rem;
      border-radius: 6px;
      overflow-x: auto;
    }
    a {
      color: #0077cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>PRISM: Performer RS-IMLE for Multisensory Imitation Learning</h1>
    <p><strong>Anonymous Submission </strong></p>

    <h2>Abstract</h2>
    <p>
      Robotic imitation learning typically requires models that capture multimodal action distributions while operating in real-time control rates and accommodating
multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation
(IMLE) have achieved promising results in this domain, they satisfy only a subset of these requirements. To satisfy these requirements, we introduce PRISM, based
on a batch-global rejection-sampling variant of IMLE. PRISM is a single-pass policy that couples a temporal multisensory encoder (e.g, RGB, Depth, tactile,
audio, proprioception) with a linear-attention generator using a Performer architecture. We validate on MetaWorld, CALVIN, Robomimic, and a real hardware
suite using a Unitree GO2 with a 7-DoF arm, wrist and shoulder RGB, tactile, audio, and proprioception sensors. PRISM matches or outperforms diffusion,
flow-matching, and prior IMLE policies in terms of task success rates, robustness, and sample efficiency. In CALVIN with 10% of the data, PRISM improves
the success rate by ∼10% over IMLE, ∼20% over flow matching, and ∼25% over diffusion, while reducing the jerk by about 20×. On MetaWorld, PRISM is 5-12% on Hard/Very-Hard splits over diffusion and flow baselines. Real-world
loco-manipulation shows 10–25% higher success and maintains faster inference compared to diffusion policy. These results position PRISM as a fast, accurate,
and multisensory imitation policy that retains multimodal action coverage without iterative sampling.
    </p>

    <h2>Method Overview</h2>
    <img src="static/figure1.png" alt="PRISM model overview figure" />

    <h2>Simulation and Real-World Videos</h2>
    <div class="video-grid">
      <!-- Replace these with your actual local file paths or YouTube embeds -->
      <video controls>
        <source src="static/demo_simulation.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>

      <video controls>
        <source src="static/demo_realworld1.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>

      <!-- Optional YouTube embed -->
      <!--
      <iframe src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allowfullscreen></iframe>
      -->
    </div>

    <h2>Resources</h2>
    <ul>
      <li><a href="https://anonymous2271106-a11y.github.io/PRISM/" target="_blank">Project Page</a></li>
      <li><a href="https://github.com/anonymous2271106-a11y/PRISM" target="_blank">Code (GitHub)</a></li>
      <li><a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank">Paper (ArXiv)</a></li>
      <li><a href="https://www.youtube.com/watch?v=YOUR_VIDEO_ID" target="_blank">Video</a></li>
    </ul>

    <h2>BibTeX</h2>
    <pre>
@inproceedings{anonymous2026prism,
  title     = {PRISM: Performer RS-IMLE for Multisensory Imitation Learning},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2026}
}
    </pre>
  </div>
</body>
</html>
